{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87621145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dependencies and Imports\n",
    "import os\n",
    "import random\n",
    "\n",
    "torch_import = True\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Mapping folder names to actual symbol characters\n",
    "symbol_map = {'+': '+', '-': '-', 'div': '/', 'times': '*'}\n",
    "\n",
    "def class_to_symbol(c):\n",
    "    \"\"\"Convert a class name to its corresponding symbol (or return digits unchanged).\"\"\"\n",
    "    return symbol_map.get(c, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96caf4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected classes: ['+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '=', 'div', 'times']\n"
     ]
    }
   ],
   "source": [
    "# 2. Dataset Loading using ImageFolder\n",
    "# Directory containing subfolders for each symbol class (digits and ops)\n",
    "data_dir = './extracted_images'\n",
    "\n",
    "# Data transforms for 28x28 grayscale normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transform)\n",
    "classes = dataset.classes  # e.g. ['0','1',...,'plus','minus','div','times']\n",
    "num_classes = len(classes)\n",
    "print(\"Detected classes:\", classes)\n",
    "\n",
    "# Split into train/test\n",
    "total = len(dataset)\n",
    "train_size = int(0.8 * total)\n",
    "test_size = total - train_size\n",
    "train_ds, test_ds = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Build mapping from class names to image file paths for demos\n",
    "class_to_paths = defaultdict(list)\n",
    "for path, idx in dataset.samples:\n",
    "    cls = classes[idx]\n",
    "    class_to_paths[cls].append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83d95e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Model Definition\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "# Initialize model and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleCNN(num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1556c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training and Testing Functions\n",
    "def train(model, device, loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = nn.functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch} [{batch_idx*len(data)}/{len(loader.dataset)}] Loss={loss.item():.4f}\")\n",
    "\n",
    "\n",
    "def test(model, device, loader):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += nn.functional.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(loader.dataset)\n",
    "    accuracy = 100. * correct / len(loader.dataset)\n",
    "    print(f\"Test: Avg loss={test_loss:.4f}, Accuracy={accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e22eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Random Equation Generation and Solving\n",
    "def generate_and_solve_random_equations(model, device, class_to_paths, classes, transform, num_eq=5):\n",
    "    digits = [c for c in classes if c.isdigit()]\n",
    "    ops    = [c for c in classes if not c.isdigit()]\n",
    "    for _ in range(num_eq):\n",
    "        a_cls = random.choice(digits)\n",
    "        b_cls = random.choice(digits)\n",
    "        op_cls = random.choice(ops)\n",
    "        paths = [random.choice(class_to_paths[c]) for c in (a_cls, op_cls, b_cls)]\n",
    "\n",
    "        def predict(path):\n",
    "            img = Image.open(path).convert('L')\n",
    "            img = transform(img).unsqueeze(0).to(device)\n",
    "            with torch.no_grad(): out = model(img)\n",
    "            pred_cls = classes[out.argmax(dim=1).item()]\n",
    "            return class_to_symbol(pred_cls)\n",
    "\n",
    "        da = predict(paths[0])\n",
    "        dop = predict(paths[1])\n",
    "        db = predict(paths[2])\n",
    "        expr_true = f\"{class_to_symbol(a_cls)}{class_to_symbol(op_cls)}{class_to_symbol(b_cls)}\"\n",
    "        expr_det  = f\"{da}{dop}{db}\"\n",
    "        try: res_true = eval(expr_true)\n",
    "        except: res_true = None\n",
    "        try: res_det = eval(expr_det)\n",
    "        except: res_det = None\n",
    "        print(f\"True: {expr_true}={res_true} | Detected: {expr_det}={res_det}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c32f3f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Webcam OCR Function\n",
    "def run_webcam(model, device, transform, classes):\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        boxes = sorted([cv2.boundingRect(cnt) for cnt in contours if cv2.contourArea(cnt)>100], key=lambda b: b[0])\n",
    "        expr = ''\n",
    "        for x,y,w,h in boxes:\n",
    "            roi = cv2.resize(thresh[y:y+h, x:x+w], (28,28))\n",
    "            roi = (255-roi).astype(np.float32)/255\n",
    "            tensor = torch.from_numpy((roi-0.5)/0.5).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            with torch.no_grad(): out = model(tensor)\n",
    "            pred_cls = classes[out.argmax(dim=1).item()]\n",
    "            ch = class_to_symbol(pred_cls)\n",
    "            expr += ch\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),1)\n",
    "            cv2.putText(frame,ch,(x,y-10),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),2)\n",
    "        try: res = eval(expr); disp=f\"{expr}={res}\"\n",
    "        except: disp = expr\n",
    "        cv2.putText(frame,disp,(10,30),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2)\n",
    "        cv2.imshow('Math OCR', frame)\n",
    "        if cv2.waitKey(1)&0xFF==ord('q'): break\n",
    "    cap.release(); cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a2caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [0/136471] Loss=2.7081\n",
      "Epoch 1 [6400/136471] Loss=0.6946\n",
      "Epoch 1 [12800/136471] Loss=0.5728\n",
      "Epoch 1 [19200/136471] Loss=0.1383\n",
      "Epoch 1 [25600/136471] Loss=0.1427\n",
      "Epoch 1 [32000/136471] Loss=0.3257\n",
      "Epoch 1 [38400/136471] Loss=0.3172\n",
      "Epoch 1 [44800/136471] Loss=0.1324\n",
      "Epoch 1 [51200/136471] Loss=0.3750\n",
      "Epoch 1 [57600/136471] Loss=0.1909\n",
      "Epoch 1 [64000/136471] Loss=0.2554\n",
      "Epoch 1 [70400/136471] Loss=0.0453\n",
      "Epoch 1 [76800/136471] Loss=0.4872\n",
      "Epoch 1 [83200/136471] Loss=0.0396\n",
      "Epoch 1 [89600/136471] Loss=0.3049\n",
      "Epoch 1 [96000/136471] Loss=0.0623\n",
      "Epoch 1 [102400/136471] Loss=0.1631\n",
      "Epoch 1 [108800/136471] Loss=0.1720\n",
      "Epoch 1 [115200/136471] Loss=0.2666\n",
      "Epoch 1 [121600/136471] Loss=0.0684\n",
      "Epoch 1 [128000/136471] Loss=0.1364\n",
      "Epoch 1 [134400/136471] Loss=0.0749\n",
      "Test: Avg loss=0.0461, Accuracy=98.86%\n",
      "Epoch 2 [0/136471] Loss=0.0627\n",
      "Epoch 2 [6400/136471] Loss=0.0757\n",
      "Epoch 2 [12800/136471] Loss=0.0451\n",
      "Epoch 2 [19200/136471] Loss=0.0294\n",
      "Epoch 2 [25600/136471] Loss=0.0679\n",
      "Epoch 2 [32000/136471] Loss=0.0101\n",
      "Epoch 2 [38400/136471] Loss=0.2447\n",
      "Epoch 2 [44800/136471] Loss=0.1029\n",
      "Epoch 2 [51200/136471] Loss=0.1598\n",
      "Epoch 2 [57600/136471] Loss=0.1708\n",
      "Epoch 2 [64000/136471] Loss=0.1334\n",
      "Epoch 2 [70400/136471] Loss=0.1773\n",
      "Epoch 2 [76800/136471] Loss=0.0481\n",
      "Epoch 2 [83200/136471] Loss=0.0471\n",
      "Epoch 2 [89600/136471] Loss=0.0274\n",
      "Epoch 2 [96000/136471] Loss=0.1509\n",
      "Epoch 2 [102400/136471] Loss=0.1359\n",
      "Epoch 2 [108800/136471] Loss=0.0827\n",
      "Epoch 2 [115200/136471] Loss=0.0350\n",
      "Epoch 2 [121600/136471] Loss=0.0512\n",
      "Epoch 2 [128000/136471] Loss=0.0236\n",
      "Epoch 2 [134400/136471] Loss=0.1175\n",
      "Test: Avg loss=0.0283, Accuracy=99.40%\n",
      "Epoch 3 [0/136471] Loss=0.0464\n",
      "Epoch 3 [6400/136471] Loss=0.1352\n",
      "Epoch 3 [12800/136471] Loss=0.1570\n",
      "Epoch 3 [19200/136471] Loss=0.0370\n",
      "Epoch 3 [25600/136471] Loss=0.0560\n",
      "Epoch 3 [32000/136471] Loss=0.0126\n",
      "Epoch 3 [38400/136471] Loss=0.0377\n",
      "Epoch 3 [44800/136471] Loss=0.1641\n",
      "Epoch 3 [51200/136471] Loss=0.0667\n",
      "Epoch 3 [57600/136471] Loss=0.0803\n",
      "Epoch 3 [64000/136471] Loss=0.0869\n",
      "Epoch 3 [70400/136471] Loss=0.1127\n",
      "Epoch 3 [76800/136471] Loss=0.0346\n",
      "Epoch 3 [83200/136471] Loss=0.0740\n",
      "Epoch 3 [89600/136471] Loss=0.0772\n",
      "Epoch 3 [96000/136471] Loss=0.0445\n",
      "Epoch 3 [102400/136471] Loss=0.0352\n",
      "Epoch 3 [108800/136471] Loss=0.0632\n",
      "Epoch 3 [115200/136471] Loss=0.0170\n",
      "Epoch 3 [121600/136471] Loss=0.0495\n",
      "Epoch 3 [128000/136471] Loss=0.0280\n",
      "Epoch 3 [134400/136471] Loss=0.0148\n",
      "Test: Avg loss=0.0184, Accuracy=99.59%\n",
      "Epoch 4 [0/136471] Loss=0.0616\n",
      "Epoch 4 [6400/136471] Loss=0.0183\n",
      "Epoch 4 [12800/136471] Loss=0.1631\n",
      "Epoch 4 [19200/136471] Loss=0.0492\n",
      "Epoch 4 [25600/136471] Loss=0.0070\n",
      "Epoch 4 [32000/136471] Loss=0.0147\n",
      "Epoch 4 [38400/136471] Loss=0.0137\n",
      "Epoch 4 [44800/136471] Loss=0.0407\n",
      "Epoch 4 [51200/136471] Loss=0.0326\n",
      "Epoch 4 [57600/136471] Loss=0.0227\n",
      "Epoch 4 [64000/136471] Loss=0.0641\n",
      "Epoch 4 [70400/136471] Loss=0.0187\n",
      "Epoch 4 [76800/136471] Loss=0.0563\n",
      "Epoch 4 [83200/136471] Loss=0.0557\n",
      "Epoch 4 [89600/136471] Loss=0.0198\n",
      "Epoch 4 [96000/136471] Loss=0.0381\n",
      "Epoch 4 [102400/136471] Loss=0.0010\n",
      "Epoch 4 [108800/136471] Loss=0.0054\n",
      "Epoch 4 [115200/136471] Loss=0.0409\n",
      "Epoch 4 [121600/136471] Loss=0.0423\n",
      "Epoch 4 [128000/136471] Loss=0.0033\n",
      "Epoch 4 [134400/136471] Loss=0.0202\n",
      "Test: Avg loss=0.0158, Accuracy=99.63%\n",
      "Epoch 5 [0/136471] Loss=0.0194\n",
      "Epoch 5 [6400/136471] Loss=0.0044\n",
      "Epoch 5 [12800/136471] Loss=0.0104\n",
      "Epoch 5 [19200/136471] Loss=0.0343\n",
      "Epoch 5 [25600/136471] Loss=0.0084\n",
      "Epoch 5 [32000/136471] Loss=0.0038\n",
      "Epoch 5 [38400/136471] Loss=0.1345\n",
      "Epoch 5 [44800/136471] Loss=0.0157\n",
      "Epoch 5 [51200/136471] Loss=0.0032\n",
      "Epoch 5 [57600/136471] Loss=0.0212\n",
      "Epoch 5 [64000/136471] Loss=0.0497\n",
      "Epoch 5 [70400/136471] Loss=0.0101\n",
      "Epoch 5 [76800/136471] Loss=0.0592\n",
      "Epoch 5 [83200/136471] Loss=0.0605\n",
      "Epoch 5 [89600/136471] Loss=0.0193\n",
      "Epoch 5 [96000/136471] Loss=0.0490\n",
      "Epoch 5 [102400/136471] Loss=0.0039\n",
      "Epoch 5 [108800/136471] Loss=0.0389\n",
      "Epoch 5 [115200/136471] Loss=0.0254\n",
      "Epoch 5 [121600/136471] Loss=0.0050\n",
      "Epoch 5 [128000/136471] Loss=0.0035\n",
      "Epoch 5 [134400/136471] Loss=0.0013\n",
      "Test: Avg loss=0.0152, Accuracy=99.72%\n"
     ]
    }
   ],
   "source": [
    "# 7. Execute Workflow\n",
    "for epoch in range(1, 7):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "333dd2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Equations:\n",
      "True: 3/4=0.75 | Detected: 3/4=0.75\n",
      "True: 7+6=13 | Detected: 7+6=13\n",
      "True: 1/8=0.125 | Detected: 1/8=0.125\n",
      "True: 1+3=4 | Detected: 1+3=4\n",
      "True: 9-4=5 | Detected: 9-4=5\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nRandom Equations:\")\n",
    "generate_and_solve_random_equations(model, device, class_to_paths, classes, transform)\n",
    "\n",
    "input(\"Press Enter to start webcam OCR (q to quit)...\")\n",
    "run_webcam(model, device, transform, classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
